{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMRC Regional Trade Statistics\n",
    "\n",
    "Transform to Tidy Data.\n",
    "\n",
    "The source data is available from https://www.uktradeinfo.com/Statistics/RTS/Documents/Forms/AllItems.aspx in a series of zip files, `RTS web YYYY.zip` for the years 2013 to 2016 currently.\n",
    "\n",
    "Each zip file contains fixed-width formatted text files following a layout described in https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20Detailed%20data%20information%20pack.pdf. Each row is has two measures: net mass in tonnes and statistical value in Â£1000's. We're assuming each observation has one measure, so split these  out into separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## UK Regional Trade Statistics (RTS\n",
       "\n",
       "### Description\n",
       "\n",
       "RTS data provides a breakdown by standard UK Region geography, of the UK Overseas Trade Statistics (OTS) data collected from UK Customs import and export entries, and the HMRC Intrastat survey.\n",
       "\n",
       "### Distributions\n",
       "\n",
       "1. RTS 2013 ([application/zip](https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20web%202013.zip))\n",
       "1. RTS 2014 ([application/zip](https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20web%202014.zip))\n",
       "1. RTS 2015 ([application/zip](https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20web%202015.zip))\n",
       "1. RTS 2016 ([application/zip](https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20web%202016.zip))\n",
       "1. RTS 2017 ([application/zip](https://www.uktradeinfo.com/Statistics/RTS/Documents/Rtsweb%202017.zip))\n",
       "1. RTS 2018 ([application/zip](https://www.uktradeinfo.com/Statistics/RTS/Documents/Rtsweb%202018.zip))\n"
      ],
      "text/plain": [
       "<gssutils.scrape.Scraper at 0x11c192940>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gssutils import *\n",
    "scraper = Scraper('https://www.uktradeinfo.com/Statistics/RTS/Pages/default.aspx')\n",
    "scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20web%202013.zip\n",
      "https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20web%202014.zip\n",
      "https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20web%202015.zip\n",
      "https://www.uktradeinfo.com/Statistics/RTS/Documents/RTS%20web%202016.zip\n",
      "https://www.uktradeinfo.com/Statistics/RTS/Documents/Rtsweb%202017.zip\n",
      "https://www.uktradeinfo.com/Statistics/RTS/Documents/Rtsweb%202018.zip\n"
     ]
    }
   ],
   "source": [
    "for dist in scraper.distributions:\n",
    "    print(dist.downloadURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO, TextIOWrapper\n",
    "\n",
    "out = Path('out')\n",
    "out.mkdir(exist_ok=True, parents=True)\n",
    "extracted_files = []\n",
    "\n",
    "# For each distribution, open the zipfile and put each source in turn into a dataframe\n",
    "# for each source we're looking to create one \"value\" output, and one \"mass\" output\n",
    "for distribution in scraper.distributions:\n",
    "    with ZipFile(BytesIO(scraper.session.get(distribution.downloadURL).content)) as zip:\n",
    "        for name in zip.namelist():\n",
    "            with zip.open(name, 'r') as quarterFile:\n",
    "                quarterText = TextIOWrapper(quarterFile, encoding='utf-8')\n",
    "                table = pd.read_fwf(quarterText, widths=[6, 1, 2, 1, 3, 2, 1, 2, 9, 9], names=[\n",
    "                    'Period',\n",
    "                    'Flow',\n",
    "                    'HMRC Reporter Region',\n",
    "                    'HMRC Partner Geography',\n",
    "                    'Codalpha',\n",
    "                    'Codseq',\n",
    "                    'SITC Section',\n",
    "                    'SITC 4',\n",
    "                    'Value',\n",
    "                    'Netmass'\n",
    "                ], dtype=str)\n",
    "                \n",
    "                # Generic changes that apply to both \"mass\" and \"value\" outputs\n",
    "                table['Period'] = table['Period'].map(lambda x: f'quarter/{x[2:]}-Q{x[0]}')\n",
    "                table['Flow'] = table['Flow'].map(lambda x: 'Exports' if x == 'E' else 'Imports')\n",
    "                table['HMRC Partner Geography'] = table.apply(\n",
    "                    lambda x: x['Codseq'] if x['Codseq'][0] != '#' else x['Codalpha'],\n",
    "                    axis=1)\n",
    "                assert table['SITC Section'].equals(table['SITC 4'].apply(lambda x: x[0]))\n",
    "                table.drop(columns=['Codalpha', 'Codseq', 'SITC Section'], inplace=True)\n",
    "                \n",
    "                # Output the mass observations for this input file\n",
    "                mass = table.drop(columns=['Value'])\n",
    "                mass['Measure Type'] = 'Net Mass'\n",
    "                mass['Unit'] = 'kg-thousands'\n",
    "                mass.rename(columns={'Netmass': 'Value'}, inplace=True, index=str)\n",
    "                textFile = out / pathify(name)\n",
    "                extracted_files.append(textFile.with_suffix('.mass.csv'))\n",
    "                mass.to_csv(textFile.with_suffix('.mass.csv'), index=False)\n",
    "                \n",
    "                # Output the value observations for this input file\n",
    "                value = table.drop(columns=['Netmass'])\n",
    "                value['Measure Type'] = 'GBP Total'\n",
    "                value['Unit'] = 'gbp-thousands'\n",
    "                extracted_files.append(textFile.with_suffix('.value.csv'))\n",
    "                value.to_csv(textFile.with_suffix('.value.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.dataset.family = 'trade'\n",
    "with open(out / 'dataset.trig', 'wb') as metadata:\n",
    "    metadata.write(scraper.generate_trig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a schema.json for each \"value\" and \"mass\" csv we've outputted\n",
    "for extracted_file in extracted_files:\n",
    "    csvw = CSVWMetadata('https://gss-cogs.github.io/ref_trade/')\n",
    "    csvw.create(extracted_file, out / f'{extracted_file.name[:-4]}-csv-schema.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
